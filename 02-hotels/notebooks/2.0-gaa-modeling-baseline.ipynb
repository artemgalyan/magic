{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPES = {\n",
    "    'date_time': 'string',\n",
    "    'site_name': 'uint8',\n",
    "    'posa_continent': 'uint8',\n",
    "    'user_location_country': 'uint8',\n",
    "    'user_location_region': 'uint16',\n",
    "    'user_location_city': 'uint16',\n",
    "    'orig_destination_distance': 'float32',\n",
    "    'user_id': 'uint32',\n",
    "    'is_mobile': 'bool',\n",
    "    'is_package': 'bool',\n",
    "    'channel': 'uint8',\n",
    "    'srch_ci': 'string',\n",
    "    'srch_co': 'string',\n",
    "    'srch_adults_cnt': 'uint8',\n",
    "    'srch_children_cnt': 'uint8',\n",
    "    'srch_rm_cnt': 'uint8',\n",
    "    'srch_destination_id': 'uint16',\n",
    "    'srch_destination_type_id': 'uint8',\n",
    "    'is_booking': 'bool',\n",
    "    'cnt': 'uint16',\n",
    "    'hotel_continent': 'uint8',\n",
    "    'hotel_country': 'uint8',\n",
    "    'hotel_market': 'uint16',\n",
    "    'hotel_cluster': 'uint8',\n",
    "}\n",
    "DATETIME_COLUMNS = ['date_time', 'srch_ci', 'srch_co']\n",
    "BOOLEAN_COLUMNS = ['is_booking', 'is_mobile', 'is_package']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyoma\\AppData\\Local\\Temp\\ipykernel_12192\\3881431213.py:22: DeprecationWarning: `replace` is deprecated. DataFrame.replace is deprecated and will be removed in a future version. Please use\n",
      "    df = df.with_columns(new_column.alias(column_name))\n",
      "instead.\n",
      "  df = df.replace(col, df[col] == 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "def map_to_polars(dtype: str):\n",
    "    conversion = {\n",
    "        'string': pl.String,\n",
    "        'uint8': pl.UInt8,\n",
    "        'uint16': pl.UInt16,\n",
    "        'uint32': pl.UInt32,\n",
    "        'float32': pl.Float32,\n",
    "        'bool': pl.UInt8 \n",
    "    }\n",
    "    return conversion[dtype]\n",
    "\n",
    "dtypes = {k: map_to_polars(v) for k, v in DTYPES.items()}\n",
    "df = pl.read_csv('../data/raw/train.csv', dtypes=dtypes)\n",
    "df = df.with_columns(\n",
    "    *[pl.col(col).str.to_datetime() for col in DATETIME_COLUMNS]\n",
    ")\n",
    "for col in BOOLEAN_COLUMNS:\n",
    "    df = df.replace(col, df[col] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'is_booking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "    co_ci_diff=(pl.col('srch_co') - pl.col('srch_ci')).dt.total_days().cast(pl.Int16),\n",
    "    ci_dt_diff=(pl.col('srch_ci') - pl.col('date_time')).dt.total_days().cast(pl.Int32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [column for column in df.columns if df[column].dtype not in [pl.Boolean, pl.String, pl.Datetime]]\n",
    "categorical_columns = [column for column in df.columns if df[column].dtype in [pl.Boolean, pl.String]]\n",
    "str_columns = [column for column in df.columns if df[column].dtype == pl.String]\n",
    "datetime_columns = [column for column in df.columns if column not in numerical_columns and column not in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['date_time'] + numerical_columns + categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(TARGET)\n",
    "y = data[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "\n",
    "Metric = Callable[[NDArray, NDArray, NDArray], float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from math import floor\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def get_interval(start: datetime, end: datetime, dt_column: pl.Series) -> pl.Series:\n",
    "    return (dt_column >= start) & (dt_column < end)\n",
    "\n",
    "def handle_nans(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[df.isna()] = np.nan\n",
    "    return df\n",
    "\n",
    "\n",
    "def blocked_cross_validation(\n",
    "    start: datetime,\n",
    "    end: datetime,\n",
    "    training_interval_len: timedelta,\n",
    "    test_interval_len: timedelta,\n",
    "    estimator: BaseEstimator,\n",
    "    x: pl.DataFrame,\n",
    "    y: pl.DataFrame,\n",
    "    dt_column: pl.Series,\n",
    "    metrics: dict[str, Metric]\n",
    ") -> tuple[dict[str, list[float]], list[BaseEstimator]]:\n",
    "    result_metrics = {key: [] for key in metrics.keys()}\n",
    "    estimators = []\n",
    "    n_intervals = floor((end - test_interval_len - start) / training_interval_len)\n",
    "\n",
    "    for i in tqdm(range(n_intervals - 1)):\n",
    "        est = clone(estimator)\n",
    "        training_dt_interval = start + i * training_interval_len, start + (i + 1) * training_interval_len\n",
    "        test_dt_interval = training_dt_interval[-1], training_dt_interval[-1] + test_interval_len\n",
    "        training_interval = get_interval(*training_dt_interval, dt_column)\n",
    "        test_interval = get_interval(*test_dt_interval, dt_column)\n",
    "        est = est.fit(\n",
    "            handle_nans(x.filter(training_interval).to_pandas()), \n",
    "            y.filter(training_interval).to_pandas()\n",
    "        )\n",
    "        x_test = handle_nans(x.filter(test_interval).to_pandas())\n",
    "        y_test = y.filter(test_interval).to_pandas()\n",
    "        test_probas_predictions = est.predict_proba(x_test)\n",
    "        test_predictions = est.predict(x_test)\n",
    "\n",
    "        for key, metric in metrics.items():\n",
    "            result_metrics[key].append(metric(y_test, test_predictions, test_probas_predictions))\n",
    "        \n",
    "        estimators.append(est)\n",
    "\n",
    "    return result_metrics, estimators        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "cat_columns = categorical_columns.copy()\n",
    "cat_columns.remove('is_booking')\n",
    "train_data = x.drop('date_time')\n",
    "\n",
    "numerical_transform = Pipeline([\n",
    "    ('NumericalImputer', SimpleImputer(strategy='median', missing_values=np.nan)),\n",
    "    ('Scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "cat_transform = Pipeline([\n",
    "    ('CategoricalImputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('OneHot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "\n",
    "base_pipeline = Pipeline([\n",
    "    (\n",
    "        'TransformingColumns',\n",
    "        ColumnTransformer([\n",
    "            ('Numerical', numerical_transform, numerical_columns),\n",
    "            ('Categorical', cat_transform, str_columns)\n",
    "        ])\n",
    "    ),\n",
    "    ('Logreg', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def accuracy(y_true: NDArray, y_pred: NDArray, probas: NDArray) -> float:\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8406db6cfe0547daa5081679757a6968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics, estimators = blocked_cross_validation(\n",
    "    x['date_time'].min(),\n",
    "    x['date_time'].max(),\n",
    "    timedelta(days=14),\n",
    "    timedelta(days=7),\n",
    "    base_pipeline,\n",
    "    x=x.drop('date_time'),\n",
    "    y=y,\n",
    "    dt_column=x['date_time'],\n",
    "    metrics={\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.9108279953448325,\n",
       "  0.8931218466758941,\n",
       "  0.9124201409617835,\n",
       "  0.9108920891163276,\n",
       "  0.9116017527201136,\n",
       "  0.9089401576668465,\n",
       "  0.9080102717031147,\n",
       "  0.9048914993040842,\n",
       "  0.9042779249667694,\n",
       "  0.9067562336331549,\n",
       "  0.9079329427639332,\n",
       "  0.9116167443577791,\n",
       "  0.9129467000754129,\n",
       "  0.9125516588969548,\n",
       "  0.9096090568026917,\n",
       "  0.9065447071094102,\n",
       "  0.9060179663120645,\n",
       "  0.9076864576148281,\n",
       "  0.9029109906915481,\n",
       "  0.9054218967921897,\n",
       "  0.9049106672750886,\n",
       "  0.9046437487606331,\n",
       "  0.9069737895986963,\n",
       "  0.9061032606911045,\n",
       "  0.9166078536544053,\n",
       "  0.9144604032342826,\n",
       "  0.9153258179818532,\n",
       "  0.9145987002182547,\n",
       "  0.9142649146434682,\n",
       "  0.9205328281151857,\n",
       "  0.9212114411111942,\n",
       "  0.9180942962682878,\n",
       "  0.9185104074732511,\n",
       "  0.9169454531360147,\n",
       "  0.9163258066107663,\n",
       "  0.9199963812247348,\n",
       "  0.9222884269909526,\n",
       "  0.9260913074462117,\n",
       "  0.9298683914169288,\n",
       "  0.9306258803078361,\n",
       "  0.9283309046966098,\n",
       "  0.9271749907304715,\n",
       "  0.9284368363698443,\n",
       "  0.9279566754834317,\n",
       "  0.9261252220911419,\n",
       "  0.925258327937737,\n",
       "  0.9252674445386428,\n",
       "  0.9271625904501627,\n",
       "  0.9288133928205968,\n",
       "  0.9286809032389906]}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magic-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
